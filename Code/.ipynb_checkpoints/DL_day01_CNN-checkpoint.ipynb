{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBM의 xavier(재비어)를 사용하면 y=wx+b에서 w의 값이 적절하게 초기화된다.\n",
    "# He(헤)의 He et al.이라는 초기화 방법이 현재까지는 가장 좋은 성능을 보인다.\n",
    "# AdamOptimizer()도 성능이 상당히 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as ft\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴 넷에서는 변수 초기화가 중요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "< 레이어 10개 각 레이어에는 500개의 뉴런이 있다고 가정하자 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "방법1) small random number => 평균0, 표준편차가 0.01인 가우시안분포를 만들겠다\n",
    "       small network(2,3개의 레이어)는 괜찮다. 하지만, 깊어질수록 0으로 수렴하는 문제가 있다. \n",
    "       w=np.random.randn(입력,출력)*0.01 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력계층 평균 0.002633 and 표준편차 0.999400\n",
      "히든 계층 1 평균 -0.000246 and 표준편차 0.720519\n",
      "히든 계층 2 평균 -0.000674 and 표준편차 0.633954\n",
      "히든 계층 3 평균 0.000734 and 표준편차 0.594694\n",
      "히든 계층 4 평균 0.000326 and 표준편차 0.576818\n",
      "히든 계층 5 평균 0.001263 and 표준편차 0.565604\n",
      "히든 계층 6 평균 -0.000083 and 표준편차 0.561472\n",
      "히든 계층 7 평균 -0.000716 and 표준편차 0.559427\n",
      "히든 계층 8 평균 -0.000640 and 표준편차 0.557508\n",
      "히든 계층 9 평균 0.000197 and 표준편차 0.555948\n",
      "히든 계층 10 평균 0.000093 and 표준편차 0.554938\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-a937b58fbccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m121\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_means\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ob-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer_mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2787\u001b[0m     return gca().plot(\n\u001b[0;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2789\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m         \"\"\"\n\u001b[0;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1666\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1667\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 270\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (10,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAAD8CAYAAADHTWCVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAL5klEQVR4nO3dYajd9X3H8fdHM1fmrI56CyVJa8ribCYD3cU5CqulbkQHyRMpCcjmCIZ2tXvQMnB0uJI+mmUrFLJ1YRPbQrVpH6yXEgm0UxzSWK9orVEy7lK3XCwzbZ1PpGrYdw/Osb1+c2/u3+Tcc037fsGF8/+f3z2/30nu+/7P//4PnFQVkn7ugvVegPRWYxRSYxRSYxRSYxRSYxRSs2oUSe5J8kKSp1e4P0k+n2QhyVNJrp38MqXpGXKkuBfYfob7bwK2jr/2Av947suS1s+qUVTVw8BPzjBkJ/ClGjkCXJbkXZNaoDRtGybwGBuBE0u2F8f7ftgHJtnL6GjCxRdf/LtXXXXVBKaXTvf444//qKpmzuZ7JxFFltm37HtHquoAcABgdna25ufnJzC9dLok/3W23zuJvz4tApuXbG8Cnp/A40rrYhJRzAF/Mv4r1PXAS1V12ksn6Xyx6sunJPcBNwCXJ1kE/gb4FYCq+gJwCLgZWABeBv5srRYrTcOqUVTV7lXuL+BjE1uRtM68oi01RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1RiE1g6JIsj3JsSQLSe5c5v53J3kwyRNJnkpy8+SXKk3HqlEkuRDYD9wEbAN2J9nWhv01cLCqrgF2Af8w6YVK0zLkSHEdsFBVx6vqVeB+YGcbU8Dbx7cvxQ+X13lsSBQbgRNLthfH+5b6NHDr+HO2DwEfX+6BkuxNMp9k/uTJk2exXGntDYkiy+yrtr0buLeqNjH6oPkvJzntsavqQFXNVtXszMzMm1+tNAVDolgENi/Z3sTpL4/2AAcBquo7wNuAyyexQGnahkTxGLA1yZYkFzE6kZ5rY/4b+BBAkvcxisLXRzovrRpFVZ0C7gAOA88y+ivT0ST7kuwYD/skcHuS7wH3AbdVVX+JJZ0XNgwZVFWHGJ1AL91315LbzwDvn+zSpPXhFW2pMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpMQqpGRRFku1JjiVZSHLnCmM+nOSZJEeTfGWyy5SmZ9XPvEtyIbAf+ENGHx/8WJK58efcvT5mK/BXwPur6sUk71yrBUtrbciR4jpgoaqOV9WrwP3AzjbmdmB/Vb0IUFUvTHaZ0vQMiWIjcGLJ9uJ431JXAlcmeSTJkSTbl3ugJHuTzCeZP3nSj9nWW9OQKLLMvv4Z2RuArcANwG7gn5Ncdto3VR2oqtmqmp2ZmXmza5WmYkgUi8DmJdubgOeXGfONqnqtqn4AHGMUiXTeGRLFY8DWJFuSXATsAubamH8FPgiQ5HJGL6eOT3Kh0rSsGkVVnQLuAA4DzwIHq+pokn1JdoyHHQZ+nOQZ4EHgL6vqx2u1aGktpaqfHkzH7Oxszc/Pr8vc+sWX5PGqmj2b7/WKttQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQYhdQMiiLJ9iTHkiwkufMM425JUknO6rPGpLeCVaNIciGwH7gJ2AbsTrJtmXGXAH8BPDrpRUrTNORIcR2wUFXHq+pV4H5g5zLjPgPcDfx0guuTpm5IFBuBE0u2F8f7fibJNcDmqvrmmR4oyd4k80nmT548+aYXK03DkCiyzL6fffh2kguAzwGfXO2BqupAVc1W1ezMzMzwVUpTNCSKRWDzku1NwPNLti8BrgYeSvIccD0w58m2zldDongM2JpkS5KLgF3A3Ot3VtVLVXV5VV1RVVcAR4AdVTW/JiuW1tiqUVTVKeAO4DDwLHCwqo4m2Zdkx1ovUJq2DUMGVdUh4FDbd9cKY28492VJ68cr2lJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFJjFFIzKIok25McS7KQ5M5l7v9EkmeSPJXk20neM/mlStOxahRJLgT2AzcB24DdSba1YU8As1X1O8DXgbsnvVBpWoYcKa4DFqrqeFW9CtwP7Fw6oKoerKqXx5tHGH3WtnReGhLFRuDEku3F8b6V7AEeWO6OJHuTzCeZP3ny5PBVSlM0JIoss6+WHZjcCswCn13u/qo6UFWzVTU7MzMzfJXSFA35HO1FYPOS7U3A831QkhuBTwEfqKpXJrM8afqGHCkeA7Ym2ZLkImAXMLd0QJJrgH8CdlTVC5NfpjQ9q0ZRVaeAO4DDwLPAwao6mmRfkh3jYZ8Ffh34WpInk8yt8HDSW96Ql09U1SHgUNt315LbN054XdK68Yq21BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1BiF1AyKIsn2JMeSLCS5c5n7fzXJV8f3P5rkikkvVJqWVaNIciGwH7gJ2AbsTrKtDdsDvFhVvwl8DvjbSS9UmpYhR4rrgIWqOl5VrwL3AzvbmJ3AF8e3vw58KEkmt0xpeoZ8ZPBG4MSS7UXg91YaU1WnkrwEvAP40dJBSfYCe8ebryR5+mwWPQGX09bmvL9wc//W2X7jkCiW+41fZzGGqjoAHABIMl9VswPmn7j1mvuXbd71nDvJ/Nl+75CXT4vA5iXbm4DnVxqTZANwKfCTs12UtJ6GRPEYsDXJliQXAbuAuTZmDvjT8e1bgH+rqtOOFNL5YNWXT+NzhDuAw8CFwD1VdTTJPmC+quaAfwG+nGSB0RFi14C5D5zDus/Ves39yzbves591vPGX+jSG3lFW2qMQmrWPIr1eovIgHk/keSZJE8l+XaS90xi3iFzLxl3S5JKMpE/WQ6ZN8mHx8/7aJKvTGLeIXMneXeSB5M8Mf43v3kCc96T5IWVrndl5PPjNT2V5NpBD1xVa/bF6MT8P4H3AhcB3wO2tTF/DnxhfHsX8NUpzftB4NfGtz86iXmHzj0edwnwMHAEmJ3Sc94KPAH8xnj7nVP8fz4AfHR8exvw3ATm/QPgWuDpFe6/GXiA0XW064FHhzzuWh8p1ustIqvOW1UPVtXL480jjK6/TMKQ5wzwGeBu4KdTnPd2YH9VvQhQVS9Mce4C3j6+fSmnX+t606rqYc58PWwn8KUaOQJcluRdqz3uWkex3FtENq40pqpOAa+/RWSt511qD6PfKJOw6txJrgE2V9U3JzTnoHmBK4ErkzyS5EiS7VOc+9PArUkWgUPAxyc097mu6zRD3uZxLib2FpE1mHc0MLkVmAU+cI5zDpo7yQWM3kl824TmGzTv2AZGL6FuYHRk/PckV1fV/05h7t3AvVX1d0l+n9F1raur6v/Oce5zXddp1vpIsV5vERkyL0luBD4F7KiqV85xzqFzXwJcDTyU5DlGr3XnJnCyPfTf+htV9VpV/QA4xiiSczVk7j3AQYCq+g7wNkZvFlxLg34OTjOJE60znAhtAI4DW/j5CdhvtzEf440n2genNO81jE4Ot077ObfxDzGZE+0hz3k78MXx7csZvbR4x5TmfgC4bXz7feMfzkxg7itY+UT7j3njifZ3Bz3mJH8gVljYzcB/jH8APzXet4/Rb2cY/cb4GrAAfBd475Tm/RbwP8CT46+5aT3nNnYiUQx8zgH+HngG+D6wa4r/z9uAR8bBPAn80QTmvA/4IfAao6PCHuAjwEeWPN/94zV9f+i/s2/zkBqvaEuNUUiNUUiNUUiNUUiNUUiNUUjN/wMQIhj/Uwb9tgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "D=np.random.randn(1000,500)\n",
    "hidden_layer_sizes=[500]*10\n",
    "Hs={}\n",
    "for i in range(len(hidden_layer_sizes)):\n",
    "    X=D if i == 0 else Hs[i-1] \n",
    "    fan_in=X.shape[1]\n",
    "    fan_out=hidden_layer_sizes[i]\n",
    "\n",
    "#     w=np.random.randn(fan_in,fan_out)*0.01\n",
    "#     w=np.random.randn(fan_in,fan_out) # 평균0, 표준편차1\n",
    "#     w=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in)  # --> tanh와 해야한다. Relu와 하면 잘 안됨\n",
    "    w=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in/2) # He.et 표준정규분포를 표준편차로 나눈 것을 또 2로 나눔\n",
    "    \n",
    "    H=np.dot(X,w)\n",
    "    H=np.tanh(H) # tanh는 양수면1, 음수면 -1에 가깝게 출력되고, sigmoid는 0에서 좀만 커지면 1 아니면0으로 출력된다\n",
    "    Hs[i]=H\n",
    "    \n",
    "print(\"입력계층 평균 %f and 표준편차 %f\" % (np.mean(D),np.std(D)))\n",
    "layer_means=[np.mean(H) for i,H in Hs.items()] # key, value 리턴\n",
    "layer_stds=[np.std(H) for i,H in Hs.items()]\n",
    "\n",
    "\n",
    "for i,H in Hs.items():\n",
    "    print(\"히든 계층 %d 평균 %f and 표준편차 %f\" % (i+1, layer_means[i], layer_stds[i]) )\n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.plot(Hs.keys(), layer_means, 'ob-')\n",
    "plt.xlabel('layer')\n",
    "plt.title('layer_mean')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(Hs.keys(), layer_stds, 'ob-')\n",
    "plt.xlabel('layer')\n",
    "plt.title('layer_std')\n",
    "# layer가 늘어날수록 가중치가 0에 가까워져 제대로 된 결과를 얻을 수 없음\n",
    "\n",
    "plt.figure()\n",
    "for i,H in Hs.items():\n",
    "    plt.subplot(1,len(Hs),i+1)\n",
    "    plt.hist(H.ravel(),40, range(-1,1))\n",
    "    plt.xlabel('layer'+str(i+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "결론 : 가우시안분포(종모양)에서 점점 0으로 수렴하므로 0에 가까운 선하나만 출력된다(소멸된다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "방법2) 평균0, 표준편차 1인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging : 복원추출 / 병렬 학습\n",
    "Boosting : 복원추출 + 가중치 / 순차적 학습(data학습을 통해 나온 가중치를 분배한다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # 그래프에 있는 모든 텐서를 초기화 --> 다른 텐서 실행했다면 충돌하므로 수행해준다\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "w1=tf.get_variable(\"w1\",shape=[784,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([256]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\",shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([256]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\",shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([10]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "\n",
    "hf=tf.matmul(L2,w3)+b3\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hf,labels=y))\n",
    "\n",
    "train=tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 cost: 0.298860819\n",
      "Epoch:    2 cost: 0.111154928\n",
      "Epoch:    3 cost: 0.073004453\n",
      "Epoch:    4 cost: 0.051895922\n",
      "Epoch:    5 cost: 0.040271484\n",
      "Epoch:    6 cost: 0.028815356\n",
      "Epoch:    7 cost: 0.022476952\n",
      "Epoch:    8 cost: 0.018433381\n",
      "Epoch:    9 cost: 0.016797118\n",
      "Epoch:   10 cost: 0.015668569\n",
      "Epoch:   11 cost: 0.014306479\n",
      "Epoch:   12 cost: 0.008617795\n",
      "Epoch:   13 cost: 0.010974923\n",
      "Epoch:   14 cost: 0.014094332\n",
      "Epoch:   15 cost: 0.007411794\n",
      "acc: 0.9749\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs): # 5만개 이미지 * 15번 트레이닝\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples / batch_size) # 5만/100\n",
    "        for i in range(total_batch): # 500번(1번에 100개씩 이미지를 읽겠다) => for문이 한번 다 돌면 에폭 1\n",
    "            batch_xs, batch_ys=mnist.train.next_batch(batch_size) #이미지 100개씩 읽어오기 \n",
    "            # batch_xs=[100,784], batch_ys=[100,10]\n",
    "            myfeed={x:batch_xs,y:batch_ys}\n",
    "            cv,_=sess.run([cost,train], feed_dict=myfeed) # cv는 100개의 데이터에 대한 cost\n",
    "            avg_cost+=cv/total_batch #100개 데이터 cost/500\n",
    "        print(\"Epoch:\",\"%4d\" % (epoch+1),'cost:','{:.9f}'.format(avg_cost))\n",
    "#     print(\"learning finished\")\n",
    "#     print(\"accuarcy:\" ,sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "c_pre=tf.equal(tf.argmax(hf,1),tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre,tf.float32))\n",
    "print(\"acc:\",sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # 그래프에 있는 모든 텐서를 초기화 --> 다른 텐서 실행했다면 충돌하므로 수행해준다\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "w1=tf.get_variable(\"w1\",shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "\n",
    "w4=tf.get_variable(\"w4\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,w4)+b4)\n",
    "\n",
    "w5=tf.get_variable(\"w5\",shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "L5=tf.nn.relu(tf.matmul(L4,w5)+b5)\n",
    "hf=tf.matmul(L4,w5)+b5\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hf,labels=y))\n",
    "\n",
    "train=tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 cost: 0.301886335\n",
      "Epoch:    2 cost: 0.107392004\n",
      "Epoch:    3 cost: 0.075444499\n",
      "Epoch:    4 cost: 0.053973506\n",
      "Epoch:    5 cost: 0.044121138\n",
      "Epoch:    6 cost: 0.032899671\n",
      "Epoch:    7 cost: 0.031268230\n",
      "Epoch:    8 cost: 0.027961754\n",
      "Epoch:    9 cost: 0.021778238\n",
      "Epoch:   10 cost: 0.022377897\n",
      "Epoch:   11 cost: 0.017856162\n",
      "Epoch:   12 cost: 0.017786008\n",
      "Epoch:   13 cost: 0.016541631\n",
      "Epoch:   14 cost: 0.015126533\n",
      "Epoch:   15 cost: 0.012891133\n",
      "acc: 0.9828\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs): # 5만개 이미지 * 15번 트레이닝\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples / batch_size) # 5만/100\n",
    "        for i in range(total_batch): # 500번(1번에 100개씩 이미지를 읽겠다) => for문이 한번 다 돌면 에폭 1\n",
    "            batch_xs, batch_ys=mnist.train.next_batch(batch_size) #이미지 100개씩 읽어오기 \n",
    "            # batch_xs=[100,784], batch_ys=[100,10]\n",
    "            myfeed={x:batch_xs,y:batch_ys}\n",
    "            cv,_=sess.run([cost,train], feed_dict=myfeed) # cv는 100개의 데이터에 대한 cost\n",
    "            avg_cost+=cv/total_batch #100개 데이터 cost/500\n",
    "        print(\"Epoch:\",\"%4d\" % (epoch+1),'cost:','{:.9f}'.format(avg_cost))\n",
    "#     print(\"learning finished\")\n",
    "#     print(\"accuarcy:\" ,sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "c_pre=tf.equal(tf.argmax(hf,1),tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre,tf.float32))\n",
    "print(\"acc:\",sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # 그래프에 있는 모든 텐서를 초기화 --> 다른 텐서 실행했다면 충돌하므로 수행해준다\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "w1=tf.get_variable(\"w1\",shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "\n",
    "w4=tf.get_variable(\"w4\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,w4)+b4)\n",
    "\n",
    "w5=tf.get_variable(\"w5\",shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "L5=tf.nn.relu(tf.matmul(L4,w5)+b5)\n",
    "hf=tf.matmul(L4,w5)+b5\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hf,labels=y))\n",
    "\n",
    "train=tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 cost: 0.301886335\n",
      "Epoch:    2 cost: 0.107392004\n",
      "Epoch:    3 cost: 0.075444499\n",
      "Epoch:    4 cost: 0.053973506\n",
      "Epoch:    5 cost: 0.044121138\n",
      "Epoch:    6 cost: 0.032899671\n",
      "Epoch:    7 cost: 0.031268230\n",
      "Epoch:    8 cost: 0.027961754\n",
      "Epoch:    9 cost: 0.021778238\n",
      "Epoch:   10 cost: 0.022377897\n",
      "Epoch:   11 cost: 0.017856162\n",
      "Epoch:   12 cost: 0.017786008\n",
      "Epoch:   13 cost: 0.016541631\n",
      "Epoch:   14 cost: 0.015126533\n",
      "Epoch:   15 cost: 0.012891133\n",
      "acc: 0.9828\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs): # 5만개 이미지 * 15번 트레이닝\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples / batch_size) # 5만/100\n",
    "        for i in range(total_batch): # 500번(1번에 100개씩 이미지를 읽겠다) => for문이 한번 다 돌면 에폭 1\n",
    "            batch_xs, batch_ys=mnist.train.next_batch(batch_size) #이미지 100개씩 읽어오기 \n",
    "            # batch_xs=[100,784], batch_ys=[100,10]\n",
    "            myfeed={x:batch_xs,y:batch_ys}\n",
    "            cv,_=sess.run([cost,train], feed_dict=myfeed) # cv는 100개의 데이터에 대한 cost\n",
    "            avg_cost+=cv/total_batch #100개 데이터 cost/500\n",
    "        print(\"Epoch:\",\"%4d\" % (epoch+1),'cost:','{:.9f}'.format(avg_cost))\n",
    "#     print(\"learning finished\")\n",
    "#     print(\"accuarcy:\" ,sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "c_pre=tf.equal(tf.argmax(hf,1),tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre,tf.float32))\n",
    "print(\"acc:\",sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop out : 매 층에서 특징을 추출하여 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # 그래프에 있는 모든 텐서를 초기화 --> 다른 텐서 실행했다면 충돌하므로 수행해준다\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "w1=tf.get_variable(\"w1\",shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "\n",
    "w2=tf.get_variable(\"w2\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "L2=tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "\n",
    "w3=tf.get_variable(\"w3\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "L3=tf.nn.dropout(L3,keep_prob=keep_prob)\n",
    "\n",
    "w4=tf.get_variable(\"w4\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,w4)+b4)\n",
    "L4=tf.nn.dropout(L4,keep_prob=keep_prob)\n",
    "\n",
    "w5=tf.get_variable(\"w5\",shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "L5=tf.nn.relu(tf.matmul(L4,w5)+b5)\n",
    "hf=tf.matmul(L4,w5)+b5\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hf,labels=y))\n",
    "\n",
    "train=tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 cost: 0.459282274\n",
      "Epoch:    2 cost: 0.169935594\n",
      "Epoch:    3 cost: 0.128616756\n",
      "Epoch:    4 cost: 0.107031673\n",
      "Epoch:    5 cost: 0.092420008\n",
      "Epoch:    6 cost: 0.083330294\n",
      "Epoch:    7 cost: 0.078245563\n",
      "Epoch:    8 cost: 0.066661082\n",
      "Epoch:    9 cost: 0.061973834\n",
      "Epoch:   10 cost: 0.060018321\n",
      "Epoch:   11 cost: 0.055969995\n",
      "Epoch:   12 cost: 0.054765219\n",
      "Epoch:   13 cost: 0.050063559\n",
      "Epoch:   14 cost: 0.047913927\n",
      "Epoch:   15 cost: 0.048316889\n",
      "acc: 0.9828\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs): # 5만개 이미지 * 15번 트레이닝\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples / batch_size) # 5만/100\n",
    "        for i in range(total_batch): # 500번(1번에 100개씩 이미지를 읽겠다) => for문이 한번 다 돌면 에폭 1\n",
    "            batch_xs, batch_ys=mnist.train.next_batch(batch_size) #이미지 100개씩 읽어오기 \n",
    "            # batch_xs=[100,784], batch_ys=[100,10]\n",
    "            myfeed={x:batch_xs,y:batch_ys,keep_prob:0.7}\n",
    "            cv,_=sess.run([cost,train], feed_dict=myfeed) # cv는 100개의 데이터에 대한 cost\n",
    "            avg_cost+=cv/total_batch #100개 데이터 cost/500\n",
    "        print(\"Epoch:\",\"%4d\" % (epoch+1),'cost:','{:.9f}'.format(avg_cost))\n",
    "#     print(\"learning finished\")\n",
    "#     print(\"accuarcy:\" ,sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "c_pre=tf.equal(tf.argmax(hf,1),tf.argmax(y,1)) \n",
    "acc=tf.reduce_mean(tf.cast(c_pre,tf.float32))\n",
    "print(\"acc:\",sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1})) # 실행할 때에는, keep_prob값을 1로 줘야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutional계층(합성곱)은 형상을 유지하는 계층이다\n",
    "합성곱계층의 입력은 특징입력맵 출력은 특징출력맵이라고 한다\n",
    "합성곱연산이란 ? element wide product를 수행하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "입력data \n",
    "출력data=필터(or 커널): 신경망에선 weight의 역할을 한다\n",
    "        윈도우란 입력데이터층에서 필터가 씌여지는 부분\n",
    "--> 합성곱을 수행(stride:우측 아래로 스캐닝하듯이) 하면 입력data 위에 필터를 겹쳐 올려서 합성곱을 수행한다\n",
    "--> 합성곱의 결과는 필터에서 뽑아내는 특징과 관련된 특징값이다.\n",
    "    ex) 얼굴(입력data)에 주름추출filter를 사용해서 얻어지는 값은 주름에 대한 특징 값이다.\n",
    "이때, CNN 기법을 사용하면 계산량이 확연하게 줄어든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "윈도우를 얼만큼씩 이동시킬지(stride)는 프로그래머가 결정해주어야한다.\n",
    "입력data size(N*N)이고 윈도우 사이즈가 (m*m)일때, stride=s면\n",
    "결과data size 는 N-m/(s)+1로 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " y=wx+b에서 \n",
    "    w: 입력신호가 출력 결과에 주는 영향도를 조절하는 변수\n",
    "    b: 뉴런이 얼마나 쉽게 활성화 되느냐를 나타내는 매개 변수(편향)\n",
    "        \n",
    "ex) 윈도우 3*3일때, 가중치:9이고 편향:1이다. \n",
    "    커널(필터)의 값은 모든 윈도우에 공통으로 적용된다.\n",
    "    필터 => 어떤 특징이 해당 영역(윈도우)에 있는지 없는지를 검출하는 함수\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpooling:특징이 가장 크게 드러나게 하는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding: 필터주변을 특정값으로 채워주는 것(패딩1=두께1인 패딩) \n",
    "    --> 출력에 대한 크기를 조정하기 위한 용도이다(합성곱 수행할 수록 크기가 작아지기 때문이다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride:출력 크기 간격\n",
    "    stride가 크면, 출력 크기가 줄어들고\n",
    "    padding이 크면, 출력 크기가 커진다\n",
    "    \n",
    "입력크기(H,W) 출력크기(OH,OW) 필터크기(FH,FW) 패팅크기(P*P) stride S일 때,\n",
    "일반적인 관계식\n",
    "OH=(H+2P-FH)/S+1\n",
    "OW=(W+2P-FW)/S+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling도 크기를 결정해주어야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,28*28])\n",
    "ximg=tf.reshape(x,[-1,28,28,1]) # black/white이므로 맨뒤에 1임 \n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "w1=tf.Variable(tf.random_normal([3,3,1,32])) # 3행*3렬*1뎁쓰의 필터를 32개 사용\n",
    "L1=tf.nn.conv2d(ximg,w1,strides=[1,1,1,1],padding=\"SAME\")  # stride=[1,a,b,1]에서 맨끝의 1은 고정이고, a는 좌우로 움직이는 크기 b는 아래로 움직이는 크기\n",
    "L1=tf.nn.relu(L1)\n",
    "L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\") # 2*2의 풀링하려는 행렬을 만듬\n",
    "# L1이미지 ?*28*28*1\n",
    "# conv--> (?,28,28,32)\n",
    "# pooling-->(?,14,14,32)\n",
    "\n",
    "w2=tf.Variable(tf.random_normal([3,3,32,64])) # 3행*3렬*1뎁쓰의 필터를 32개 사용\n",
    "L2=tf.nn.conv2d(L1,w2,strides=[1,1,1,1],padding=\"SAME\")  # stride=[1,a,b,1]에서 맨끝의 1은 고정이고, a는 좌우로 움직이는 크기 b는 아래로 움직이는 크기\n",
    "L2=tf.nn.relu(L2)\n",
    "L2=tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\") # 2*2의 풀링하려는 행렬을 만듬\n",
    "# conv2d-->(?,14,14,64)\n",
    "# relu-->(?,14,14,64)\n",
    "# max_pool-->(?,7,7,64)\n",
    "\n",
    "L2_flat=tf.reshape(L2,[-1,7*7*64])\n",
    "# reshape-->(?,7*7*64)\n",
    "\n",
    "w3=tf.get_variable(\"w3\",shape=[7*7*64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b=tf.Variable(tf.random_normal([10]))\n",
    "logits=(tf.matmul(L2_flat,w3)+b)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# tf.Variable(tf.random_normal([3,3,1,32])) # 3행*3렬*1뎁쓰의 필터를 32개 사용\n",
    "# L1=tf.nn.conv2d(ximg,w1,strides=[1,1,1,1],padding=\"SAME\")  # stride=[1,a,b,1]에서 맨끝의 1은 고정이고, a는 좌우로 움직이는 크기 b는 아래로 움직이는 크기\n",
    "# L1=tf.nn.relu(L1)\n",
    "# L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\") # 2*2의 풀링하려는 행렬을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 cost: 0.872398866\n",
      "Epoch:    2 cost: 0.221885034\n",
      "Epoch:    3 cost: 0.137596160\n",
      "Epoch:    4 cost: 0.093039030\n",
      "Epoch:    5 cost: 0.066833730\n",
      "Epoch:    6 cost: 0.055998832\n",
      "Epoch:    7 cost: 0.050047222\n",
      "Epoch:    8 cost: 0.041411372\n",
      "Epoch:    9 cost: 0.024919725\n",
      "Epoch:   10 cost: 0.025519766\n",
      "Epoch:   11 cost: 0.021879144\n",
      "Epoch:   12 cost: 0.026323417\n",
      "Epoch:   13 cost: 0.018063539\n",
      "Epoch:   14 cost: 0.016202695\n",
      "Epoch:   15 cost: 0.014912531\n",
      "acc: 0.9851\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs): # 5만개 이미지 * 15번 트레이닝\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples / batch_size) # 5만/100\n",
    "        for i in range(total_batch): # 500번(1번에 100개씩 이미지를 읽겠다) => for문이 한번 다 돌면 에폭 1\n",
    "            batch_xs, batch_ys=mnist.train.next_batch(batch_size) #이미지 100개씩 읽어오기             \n",
    "            myfeed={x:batch_xs,y:batch_ys}\n",
    "            cv,_=sess.run([cost,train], feed_dict=myfeed) # cv는 100개의 데이터에 대한 cost\n",
    "            avg_cost+=cv/total_batch #100개 데이터 cost/500\n",
    "        print(\"Epoch:\",\"%4d\" % (epoch+1),'cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre,tf.float32))\n",
    "print(\"acc:\",sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels})) # 실행할 때에는, keep_prob값을 1로 줘야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
