{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 변환작업 순서\n",
    "1. 표준화(이상치 처리)\n",
    "2. 정규화(0~1)\n",
    "3. 이산화(binarizer, onehotencoder)\n",
    "4. 변수 갯수 축소(selection)\n",
    "5. 차원 감소(pca)\n",
    "6. 시그널 변환 (퓨리에변환, 웨이브릿 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizer(class), Binarize(method) \n",
    "연속형 변수가 기준값보다 작다면, 0\n",
    "연속형 변수가 기준값보다 크거나 같다면, 1 로 변환해주는 클래스, 함수\n",
    "\n",
    "## onehotencoder()\n",
    "범주형 변수를 이항변수화 시켜주는 인코더\n",
    "Male은 1 Female은 0 / 학점 변환.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standardScaler() :표준화\n",
    "(각 데이터-평균)/ 표준편차\n",
    "\n",
    "## minmaxScaler() : 정규화\n",
    "(각데이터-최솟값)/(최댓값-최솟값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 더미변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overfitting\n",
    ":training data에 과하게 \n",
    "--> 1) Data는 많을수록 좋다. \n",
    "2) 변수는 적당한게 좋다(비슷한 부류는 묶어 주는것도 좋은 방법)\n",
    "3) 표준화 작업이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-c5a7890cb916>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-c5a7890cb916>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    여러개의 레이블을 분류하여 가장 큰 값을 찾는 개념으로, 뉴런의 출력 값을 정규화\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Softmax Cross-Entropy\n",
    "여러개의 레이블을 분류하여 가장 큰 값을 찾는 개념으로, 뉴런의 출력 값을 정규화\n",
    "소프트맥스를 통해 얻은 값은 결과적으로 모든 분류기에서 나온 값에 대한 확률에 해당한다.\n",
    "\n",
    "* 시그모이드 함수처럼 0~1사이 값으로 만들어주고, \n",
    "* 총 합이 1이된다\n",
    "--> 0.1/0.7/0.2과 같은 숫자를 얻고자 함이 아닌 \n",
    "    그래서 0.7에 해당하는 분류가 무엇인가를 구하고자함\n",
    "\n",
    "* 따라서, 주로 소프트맥스를 통해 얻은 값을 원핫인코딩하여 가장 큰 값을 반환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "예를 들어\n",
    "3에 대한 이미지를 어떤 모델 분류기에 넣었을 때,\n",
    "1에 대한 분류기값:0.05\n",
    "2에 대한 분류기값:0.1\n",
    "3에 대한 분류기값:0.6  \n",
    "    ...\n",
    "이렇게 각 분류기를 나온 값은 score(Logit)이라고 한다.\n",
    "\n",
    "이 값을 소프트 분류기에 넣으면 확률로 변환되어 출력된다. (이 값이 예측값)\n",
    "\n",
    "이때, 분류기에 대한 update가 \n",
    "cost함수와 차이가 최소가 되는 쪽으로 업데이트 되어야하는데(분류기 모두 같이 업데이트 되어야한다)\n",
    "cost func : D(S-Y)= -총합(실제값*로그(예측값))  --> 로지스틱 함수의 계산과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Softmax의 cost Function\n",
    "cost func : D(S-Y)= -총합(실제값*로그(예측값)) \n",
    "--> tensorFlow에는 이 함수가 구현되어있다. \n",
    "    softmaxcrossentrioywithlogit() : score값만 넘겨주면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mlt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyScaler(data): #minmaxscaling을 하는 함수\n",
    "#     print(np.min(data)) # 데이터 전체 최솟값\n",
    "#     print(np.min(data,axis=0)) # 데이터 열 단위의 최솟값\n",
    "#     print(np.min(data,axis=1)) # 데이터 행 단위의 최솟값\n",
    "    de = np.max(data,axis=0)-np.min(data,axis=0)\n",
    "    num= data-np.min(data,axis=0)\n",
    "    return num/de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy=MyScaler(xdata)\n",
    "# xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        ],\n",
       "       [0.83755792],\n",
       "       [0.6606331 ],\n",
       "       [0.43800918],\n",
       "       [0.42624401],\n",
       "       [0.49276137],\n",
       "       [0.18597238],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata=xy[:,0:-1]\n",
    "xdata\n",
    "ydata=xy[:,[-1]]\n",
    "ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,4])\n",
    "y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "w=tf.Variable(tf.random_normal([4,1]))\n",
    "b=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.matmul(x,w)+b\n",
    "cost=tf.reduce_mean(tf.square(hf-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 1.452138 hf: [[-0.8740607 ]\n",
      " [-0.8518066 ]\n",
      " [-0.71226406]\n",
      " [-0.50375044]\n",
      " [-0.6663349 ]\n",
      " [-0.5050605 ]\n",
      " [-0.3513463 ]\n",
      " [-0.03467229]]\n",
      "1 cost: 1.4520335 hf: [[-0.87399995]\n",
      " [-0.8517489 ]\n",
      " [-0.7122161 ]\n",
      " [-0.5037135 ]\n",
      " [-0.6662908 ]\n",
      " [-0.505018  ]\n",
      " [-0.35131866]\n",
      " [-0.0346455 ]]\n",
      "2 cost: 1.4519289 hf: [[-0.8739392 ]\n",
      " [-0.851691  ]\n",
      " [-0.7121681 ]\n",
      " [-0.5036765 ]\n",
      " [-0.66624683]\n",
      " [-0.5049755 ]\n",
      " [-0.351291  ]\n",
      " [-0.03461872]]\n",
      "3 cost: 1.4518242 hf: [[-0.8738784 ]\n",
      " [-0.8516333 ]\n",
      " [-0.7121201 ]\n",
      " [-0.50363946]\n",
      " [-0.6662027 ]\n",
      " [-0.5049331 ]\n",
      " [-0.35126334]\n",
      " [-0.03459194]]\n",
      "4 cost: 1.4517195 hf: [[-0.8738176 ]\n",
      " [-0.85157555]\n",
      " [-0.7120721 ]\n",
      " [-0.50360245]\n",
      " [-0.6661586 ]\n",
      " [-0.50489056]\n",
      " [-0.3512357 ]\n",
      " [-0.03456517]]\n",
      "5 cost: 1.451615 hf: [[-0.8737569 ]\n",
      " [-0.85151786]\n",
      " [-0.7120241 ]\n",
      " [-0.50356543]\n",
      " [-0.66611457]\n",
      " [-0.5048481 ]\n",
      " [-0.35120806]\n",
      " [-0.03453839]]\n",
      "6 cost: 1.4515104 hf: [[-0.87369615]\n",
      " [-0.8514602 ]\n",
      " [-0.7119762 ]\n",
      " [-0.5035285 ]\n",
      " [-0.6660706 ]\n",
      " [-0.5048057 ]\n",
      " [-0.35118037]\n",
      " [-0.03451163]]\n",
      "7 cost: 1.451406 hf: [[-0.8736354 ]\n",
      " [-0.8514025 ]\n",
      " [-0.7119282 ]\n",
      " [-0.5034915 ]\n",
      " [-0.6660265 ]\n",
      " [-0.50476325]\n",
      " [-0.35115275]\n",
      " [-0.03448486]]\n",
      "8 cost: 1.4513015 hf: [[-0.8735747 ]\n",
      " [-0.8513448 ]\n",
      " [-0.7118802 ]\n",
      " [-0.5034545 ]\n",
      " [-0.6659824 ]\n",
      " [-0.5047208 ]\n",
      " [-0.35112512]\n",
      " [-0.03445808]]\n",
      "9 cost: 1.4511969 hf: [[-0.87351394]\n",
      " [-0.8512871 ]\n",
      " [-0.7118322 ]\n",
      " [-0.50341755]\n",
      " [-0.6659384 ]\n",
      " [-0.5046783 ]\n",
      " [-0.35109746]\n",
      " [-0.03443131]]\n",
      "10 cost: 1.4510924 hf: [[-0.87345314]\n",
      " [-0.8512294 ]\n",
      " [-0.71178424]\n",
      " [-0.50338054]\n",
      " [-0.66589427]\n",
      " [-0.5046359 ]\n",
      " [-0.3510698 ]\n",
      " [-0.03440455]]\n",
      "11 cost: 1.4509878 hf: [[-0.8733924 ]\n",
      " [-0.85117173]\n",
      " [-0.7117363 ]\n",
      " [-0.5033436 ]\n",
      " [-0.6658503 ]\n",
      " [-0.50459343]\n",
      " [-0.35104215]\n",
      " [-0.03437778]]\n",
      "12 cost: 1.4508832 hf: [[-0.87333167]\n",
      " [-0.8511139 ]\n",
      " [-0.7116883 ]\n",
      " [-0.5033066 ]\n",
      " [-0.6658062 ]\n",
      " [-0.50455093]\n",
      " [-0.35101452]\n",
      " [-0.03435101]]\n",
      "13 cost: 1.4507788 hf: [[-0.8732709 ]\n",
      " [-0.8510562 ]\n",
      " [-0.71164036]\n",
      " [-0.50326955]\n",
      " [-0.6657621 ]\n",
      " [-0.50450855]\n",
      " [-0.35098687]\n",
      " [-0.03432425]]\n",
      "14 cost: 1.4506742 hf: [[-0.8732102 ]\n",
      " [-0.8509986 ]\n",
      " [-0.7115923 ]\n",
      " [-0.50323266]\n",
      " [-0.6657181 ]\n",
      " [-0.50446606]\n",
      " [-0.35095924]\n",
      " [-0.03429749]]\n",
      "15 cost: 1.4505696 hf: [[-0.87314945]\n",
      " [-0.8509408 ]\n",
      " [-0.7115444 ]\n",
      " [-0.50319564]\n",
      " [-0.6656741 ]\n",
      " [-0.5044236 ]\n",
      " [-0.35093158]\n",
      " [-0.03427072]]\n",
      "16 cost: 1.4504652 hf: [[-0.8730887 ]\n",
      " [-0.8508832 ]\n",
      " [-0.71149635]\n",
      " [-0.5031586 ]\n",
      " [-0.66563   ]\n",
      " [-0.5043812 ]\n",
      " [-0.35090396]\n",
      " [-0.03424396]]\n",
      "17 cost: 1.4503608 hf: [[-0.873028  ]\n",
      " [-0.8508255 ]\n",
      " [-0.71144843]\n",
      " [-0.50312173]\n",
      " [-0.66558594]\n",
      " [-0.50433874]\n",
      " [-0.3508763 ]\n",
      " [-0.0342172 ]]\n",
      "18 cost: 1.4502562 hf: [[-0.87296724]\n",
      " [-0.8507678 ]\n",
      " [-0.7114005 ]\n",
      " [-0.50308466]\n",
      " [-0.6655419 ]\n",
      " [-0.50429624]\n",
      " [-0.35084867]\n",
      " [-0.03419044]]\n",
      "19 cost: 1.4501518 hf: [[-0.87290645]\n",
      " [-0.85071015]\n",
      " [-0.7113525 ]\n",
      " [-0.50304776]\n",
      " [-0.6654978 ]\n",
      " [-0.50425386]\n",
      " [-0.35082102]\n",
      " [-0.03416368]]\n",
      "20 cost: 1.4500473 hf: [[-0.87284577]\n",
      " [-0.85065246]\n",
      " [-0.71130455]\n",
      " [-0.50301075]\n",
      " [-0.6654538 ]\n",
      " [-0.5042113 ]\n",
      " [-0.35079342]\n",
      " [-0.03413692]]\n",
      "21 cost: 1.4499428 hf: [[-0.872785  ]\n",
      " [-0.85059464]\n",
      " [-0.71125656]\n",
      " [-0.50297374]\n",
      " [-0.66540974]\n",
      " [-0.5041689 ]\n",
      " [-0.35076576]\n",
      " [-0.03411017]]\n",
      "22 cost: 1.4498383 hf: [[-0.8727243 ]\n",
      " [-0.85053694]\n",
      " [-0.7112086 ]\n",
      " [-0.50293684]\n",
      " [-0.66536564]\n",
      " [-0.5041265 ]\n",
      " [-0.35073814]\n",
      " [-0.0340834 ]]\n",
      "23 cost: 1.4497337 hf: [[-0.8726635 ]\n",
      " [-0.85047925]\n",
      " [-0.71116066]\n",
      " [-0.50289977]\n",
      " [-0.6653216 ]\n",
      " [-0.504084  ]\n",
      " [-0.3507105 ]\n",
      " [-0.03405665]]\n",
      "24 cost: 1.4496294 hf: [[-0.8726028 ]\n",
      " [-0.8504216 ]\n",
      " [-0.7111126 ]\n",
      " [-0.5028628 ]\n",
      " [-0.6652776 ]\n",
      " [-0.50404155]\n",
      " [-0.35068285]\n",
      " [-0.0340299 ]]\n",
      "25 cost: 1.4495249 hf: [[-0.872542  ]\n",
      " [-0.8503639 ]\n",
      " [-0.7110647 ]\n",
      " [-0.50282586]\n",
      " [-0.6652335 ]\n",
      " [-0.5039991 ]\n",
      " [-0.3506552 ]\n",
      " [-0.03400314]]\n",
      "26 cost: 1.4494205 hf: [[-0.87248135]\n",
      " [-0.8503062 ]\n",
      " [-0.7110167 ]\n",
      " [-0.50278884]\n",
      " [-0.6651895 ]\n",
      " [-0.5039567 ]\n",
      " [-0.3506276 ]\n",
      " [-0.03397638]]\n",
      "27 cost: 1.4493163 hf: [[-0.8724206 ]\n",
      " [-0.85024863]\n",
      " [-0.7109688 ]\n",
      " [-0.502752  ]\n",
      " [-0.6651455 ]\n",
      " [-0.5039143 ]\n",
      " [-0.35059997]\n",
      " [-0.03394963]]\n",
      "28 cost: 1.4492117 hf: [[-0.87235993]\n",
      " [-0.8501909 ]\n",
      " [-0.7109209 ]\n",
      " [-0.502715  ]\n",
      " [-0.66510147]\n",
      " [-0.50387186]\n",
      " [-0.35057235]\n",
      " [-0.03392288]]\n",
      "29 cost: 1.4491073 hf: [[-0.87229913]\n",
      " [-0.85013324]\n",
      " [-0.7108729 ]\n",
      " [-0.502678  ]\n",
      " [-0.66505736]\n",
      " [-0.5038294 ]\n",
      " [-0.35054472]\n",
      " [-0.03389613]]\n",
      "30 cost: 1.449003 hf: [[-0.87223846]\n",
      " [-0.85007554]\n",
      " [-0.7108249 ]\n",
      " [-0.502641  ]\n",
      " [-0.6650134 ]\n",
      " [-0.503787  ]\n",
      " [-0.35051706]\n",
      " [-0.03386938]]\n",
      "31 cost: 1.4488982 hf: [[-0.87217766]\n",
      " [-0.8500177 ]\n",
      " [-0.7107769 ]\n",
      " [-0.502604  ]\n",
      " [-0.66496927]\n",
      " [-0.5037445 ]\n",
      " [-0.35048947]\n",
      " [-0.03384264]]\n",
      "32 cost: 1.4487941 hf: [[-0.87211704]\n",
      " [-0.84996015]\n",
      " [-0.71072906]\n",
      " [-0.5025672 ]\n",
      " [-0.6649253 ]\n",
      " [-0.5037021 ]\n",
      " [-0.3504618 ]\n",
      " [-0.03381588]]\n",
      "33 cost: 1.4486896 hf: [[-0.87205625]\n",
      " [-0.8499024 ]\n",
      " [-0.7106811 ]\n",
      " [-0.50253016]\n",
      " [-0.6648812 ]\n",
      " [-0.50365967]\n",
      " [-0.3504342 ]\n",
      " [-0.03378914]]\n",
      "34 cost: 1.448585 hf: [[-0.87199557]\n",
      " [-0.84984475]\n",
      " [-0.7106331 ]\n",
      " [-0.50249314]\n",
      " [-0.6648371 ]\n",
      " [-0.50361717]\n",
      " [-0.3504066 ]\n",
      " [-0.03376239]]\n",
      "35 cost: 1.4484808 hf: [[-0.87193483]\n",
      " [-0.8497871 ]\n",
      " [-0.7105851 ]\n",
      " [-0.5024562 ]\n",
      " [-0.6647932 ]\n",
      " [-0.5035748 ]\n",
      " [-0.35037896]\n",
      " [-0.03373564]]\n",
      "36 cost: 1.4483764 hf: [[-0.87187415]\n",
      " [-0.8497294 ]\n",
      " [-0.7105372 ]\n",
      " [-0.5024193 ]\n",
      " [-0.6647492 ]\n",
      " [-0.50353235]\n",
      " [-0.3503513 ]\n",
      " [-0.0337089 ]]\n",
      "37 cost: 1.448272 hf: [[-0.8718134 ]\n",
      " [-0.8496718 ]\n",
      " [-0.7104892 ]\n",
      " [-0.5023823 ]\n",
      " [-0.6647051 ]\n",
      " [-0.50349   ]\n",
      " [-0.3503237 ]\n",
      " [-0.03368215]]\n",
      "38 cost: 1.4481678 hf: [[-0.8717527 ]\n",
      " [-0.8496141 ]\n",
      " [-0.7104413 ]\n",
      " [-0.5023453 ]\n",
      " [-0.6646611 ]\n",
      " [-0.5034475 ]\n",
      " [-0.3502961 ]\n",
      " [-0.03365542]]\n",
      "39 cost: 1.4480634 hf: [[-0.871692  ]\n",
      " [-0.84955645]\n",
      " [-0.71039337]\n",
      " [-0.50230837]\n",
      " [-0.66461706]\n",
      " [-0.5034051 ]\n",
      " [-0.35026845]\n",
      " [-0.03362866]]\n",
      "40 cost: 1.4479591 hf: [[-0.87163126]\n",
      " [-0.8494988 ]\n",
      " [-0.7103455 ]\n",
      " [-0.5022714 ]\n",
      " [-0.664573  ]\n",
      " [-0.50336266]\n",
      " [-0.35024086]\n",
      " [-0.03360192]]\n",
      "41 cost: 1.4478545 hf: [[-0.8715705 ]\n",
      " [-0.849441  ]\n",
      " [-0.71029747]\n",
      " [-0.50223446]\n",
      " [-0.66452897]\n",
      " [-0.5033202 ]\n",
      " [-0.35021326]\n",
      " [-0.03357518]]\n",
      "42 cost: 1.4477503 hf: [[-0.87150985]\n",
      " [-0.84938335]\n",
      " [-0.7102496 ]\n",
      " [-0.5021975 ]\n",
      " [-0.664485  ]\n",
      " [-0.50327784]\n",
      " [-0.35018563]\n",
      " [-0.03354844]]\n",
      "43 cost: 1.447646 hf: [[-0.8714492 ]\n",
      " [-0.8493257 ]\n",
      " [-0.7102016 ]\n",
      " [-0.5021606 ]\n",
      " [-0.66444093]\n",
      " [-0.5032354 ]\n",
      " [-0.350158  ]\n",
      " [-0.0335217 ]]\n",
      "44 cost: 1.4475417 hf: [[-0.8713885 ]\n",
      " [-0.8492681 ]\n",
      " [-0.7101537 ]\n",
      " [-0.5021236 ]\n",
      " [-0.66439694]\n",
      " [-0.503193  ]\n",
      " [-0.35013038]\n",
      " [-0.03349497]]\n",
      "45 cost: 1.4474373 hf: [[-0.87132776]\n",
      " [-0.8492104 ]\n",
      " [-0.7101057 ]\n",
      " [-0.5020867 ]\n",
      " [-0.66435295]\n",
      " [-0.5031506 ]\n",
      " [-0.35010278]\n",
      " [-0.03346824]]\n",
      "46 cost: 1.447333 hf: [[-0.871267  ]\n",
      " [-0.84915274]\n",
      " [-0.71005774]\n",
      " [-0.5020497 ]\n",
      " [-0.6643089 ]\n",
      " [-0.50310814]\n",
      " [-0.35007516]\n",
      " [-0.0334415 ]]\n",
      "47 cost: 1.4472287 hf: [[-0.87120634]\n",
      " [-0.8490951 ]\n",
      " [-0.7100099 ]\n",
      " [-0.5020128 ]\n",
      " [-0.6642649 ]\n",
      " [-0.5030657 ]\n",
      " [-0.35004756]\n",
      " [-0.03341476]]\n",
      "48 cost: 1.4471245 hf: [[-0.8711456 ]\n",
      " [-0.84903747]\n",
      " [-0.7099619 ]\n",
      " [-0.50197583]\n",
      " [-0.66422087]\n",
      " [-0.5030233 ]\n",
      " [-0.35001996]\n",
      " [-0.03338803]]\n",
      "49 cost: 1.4470203 hf: [[-0.8710849 ]\n",
      " [-0.84897983]\n",
      " [-0.70991397]\n",
      " [-0.5019389 ]\n",
      " [-0.6641768 ]\n",
      " [-0.5029809 ]\n",
      " [-0.34999233]\n",
      " [-0.0333613 ]]\n",
      "50 cost: 1.4469159 hf: [[-0.87102425]\n",
      " [-0.8489222 ]\n",
      " [-0.70986605]\n",
      " [-0.50190187]\n",
      " [-0.66413283]\n",
      " [-0.50293845]\n",
      " [-0.34996474]\n",
      " [-0.03333459]]\n",
      "51 cost: 1.4468117 hf: [[-0.8709636 ]\n",
      " [-0.8488645 ]\n",
      " [-0.7098181 ]\n",
      " [-0.50186497]\n",
      " [-0.6640888 ]\n",
      " [-0.50289613]\n",
      " [-0.34993714]\n",
      " [-0.03330786]]\n",
      "52 cost: 1.4467072 hf: [[-0.87090284]\n",
      " [-0.8488068 ]\n",
      " [-0.7097702 ]\n",
      " [-0.501828  ]\n",
      " [-0.6640448 ]\n",
      " [-0.50285363]\n",
      " [-0.3499095 ]\n",
      " [-0.03328114]]\n",
      "53 cost: 1.4466031 hf: [[-0.87084216]\n",
      " [-0.8487492 ]\n",
      " [-0.70972234]\n",
      " [-0.5017911 ]\n",
      " [-0.66400075]\n",
      " [-0.5028113 ]\n",
      " [-0.34988192]\n",
      " [-0.03325441]]\n",
      "54 cost: 1.4464989 hf: [[-0.8707814 ]\n",
      " [-0.8486916 ]\n",
      " [-0.70967436]\n",
      " [-0.50175416]\n",
      " [-0.66395676]\n",
      " [-0.5027689 ]\n",
      " [-0.34985432]\n",
      " [-0.03322769]]\n",
      "55 cost: 1.4463947 hf: [[-0.8707208 ]\n",
      " [-0.84863394]\n",
      " [-0.7096265 ]\n",
      " [-0.5017172 ]\n",
      " [-0.66391283]\n",
      " [-0.5027265 ]\n",
      " [-0.3498267 ]\n",
      " [-0.03320097]]\n",
      "56 cost: 1.4462905 hf: [[-0.8706601 ]\n",
      " [-0.84857637]\n",
      " [-0.70957863]\n",
      " [-0.5016803 ]\n",
      " [-0.66386884]\n",
      " [-0.5026841 ]\n",
      " [-0.3497991 ]\n",
      " [-0.03317425]]\n",
      "57 cost: 1.4461862 hf: [[-0.8705994 ]\n",
      " [-0.8485187 ]\n",
      " [-0.70953065]\n",
      " [-0.5016433 ]\n",
      " [-0.6638248 ]\n",
      " [-0.5026417 ]\n",
      " [-0.3497715 ]\n",
      " [-0.03314754]]\n",
      "58 cost: 1.4460819 hf: [[-0.8705388 ]\n",
      " [-0.8484611 ]\n",
      " [-0.7094827 ]\n",
      " [-0.50160646]\n",
      " [-0.66378087]\n",
      " [-0.5025993 ]\n",
      " [-0.34974393]\n",
      " [-0.03312081]]\n",
      "59 cost: 1.4459777 hf: [[-0.87047803]\n",
      " [-0.84840345]\n",
      " [-0.7094348 ]\n",
      " [-0.50156945]\n",
      " [-0.66373676]\n",
      " [-0.50255686]\n",
      " [-0.3497163 ]\n",
      " [-0.03309409]]\n",
      "60 cost: 1.4458735 hf: [[-0.87041736]\n",
      " [-0.8483459 ]\n",
      " [-0.7093869 ]\n",
      " [-0.5015325 ]\n",
      " [-0.6636928 ]\n",
      " [-0.5025145 ]\n",
      " [-0.3496887 ]\n",
      " [-0.03306738]]\n",
      "61 cost: 1.4457693 hf: [[-0.8703567 ]\n",
      " [-0.8482881 ]\n",
      " [-0.70933896]\n",
      " [-0.50149566]\n",
      " [-0.6636488 ]\n",
      " [-0.5024721 ]\n",
      " [-0.3496611 ]\n",
      " [-0.03304066]]\n",
      "62 cost: 1.4456651 hf: [[-0.87029594]\n",
      " [-0.8482304 ]\n",
      " [-0.70929104]\n",
      " [-0.50145864]\n",
      " [-0.6636048 ]\n",
      " [-0.50242966]\n",
      " [-0.34963354]\n",
      " [-0.03301395]]\n",
      "63 cost: 1.4455608 hf: [[-0.8702353 ]\n",
      " [-0.84817284]\n",
      " [-0.7092431 ]\n",
      " [-0.5014217 ]\n",
      " [-0.66356075]\n",
      " [-0.5023873 ]\n",
      " [-0.34960592]\n",
      " [-0.03298724]]\n",
      "64 cost: 1.4454567 hf: [[-0.87017465]\n",
      " [-0.84811527]\n",
      " [-0.7091953 ]\n",
      " [-0.5013848 ]\n",
      " [-0.6635168 ]\n",
      " [-0.5023449 ]\n",
      " [-0.34957832]\n",
      " [-0.03296052]]\n",
      "65 cost: 1.4453526 hf: [[-0.8701139 ]\n",
      " [-0.84805757]\n",
      " [-0.70914733]\n",
      " [-0.50134784]\n",
      " [-0.66347283]\n",
      " [-0.50230247]\n",
      " [-0.34955075]\n",
      " [-0.03293381]]\n",
      "66 cost: 1.4452485 hf: [[-0.8700533 ]\n",
      " [-0.84800005]\n",
      " [-0.7090994 ]\n",
      " [-0.50131094]\n",
      " [-0.6634288 ]\n",
      " [-0.50226   ]\n",
      " [-0.34952313]\n",
      " [-0.03290709]]\n",
      "67 cost: 1.4451444 hf: [[-0.8699926 ]\n",
      " [-0.8479424 ]\n",
      " [-0.70905155]\n",
      " [-0.50127405]\n",
      " [-0.66338485]\n",
      " [-0.5022177 ]\n",
      " [-0.34949556]\n",
      " [-0.03288039]]\n",
      "68 cost: 1.4450402 hf: [[-0.86993194]\n",
      " [-0.8478848 ]\n",
      " [-0.7090036 ]\n",
      " [-0.5012371 ]\n",
      " [-0.66334087]\n",
      " [-0.50217533]\n",
      " [-0.34946796]\n",
      " [-0.03285367]]\n",
      "69 cost: 1.4449358 hf: [[-0.86987126]\n",
      " [-0.8478272 ]\n",
      " [-0.7089557 ]\n",
      " [-0.5012002 ]\n",
      " [-0.6632968 ]\n",
      " [-0.5021329 ]\n",
      " [-0.34944037]\n",
      " [-0.03282697]]\n",
      "70 cost: 1.4448318 hf: [[-0.8698106 ]\n",
      " [-0.84776956]\n",
      " [-0.7089078 ]\n",
      " [-0.50116324]\n",
      " [-0.66325283]\n",
      " [-0.5020905 ]\n",
      " [-0.3494128 ]\n",
      " [-0.03280026]]\n",
      "71 cost: 1.4447277 hf: [[-0.8697499 ]\n",
      " [-0.84771186]\n",
      " [-0.7088599 ]\n",
      " [-0.50112635]\n",
      " [-0.66320884]\n",
      " [-0.50204813]\n",
      " [-0.3493852 ]\n",
      " [-0.03277355]]\n",
      "72 cost: 1.4446235 hf: [[-0.8696893 ]\n",
      " [-0.8476543 ]\n",
      " [-0.708812  ]\n",
      " [-0.50108945]\n",
      " [-0.6631649 ]\n",
      " [-0.5020058 ]\n",
      " [-0.34935763]\n",
      " [-0.03274685]]\n",
      "73 cost: 1.4445194 hf: [[-0.8696286 ]\n",
      " [-0.84759665]\n",
      " [-0.7087641 ]\n",
      " [-0.5010525 ]\n",
      " [-0.66312087]\n",
      " [-0.5019634 ]\n",
      " [-0.34933   ]\n",
      " [-0.03272014]]\n",
      "74 cost: 1.4444151 hf: [[-0.8695679 ]\n",
      " [-0.84753907]\n",
      " [-0.70871615]\n",
      " [-0.50101554]\n",
      " [-0.6630769 ]\n",
      " [-0.50192094]\n",
      " [-0.34930244]\n",
      " [-0.03269345]]\n",
      "75 cost: 1.4443111 hf: [[-0.86950725]\n",
      " [-0.8474814 ]\n",
      " [-0.7086683 ]\n",
      " [-0.5009786 ]\n",
      " [-0.6630329 ]\n",
      " [-0.5018786 ]\n",
      " [-0.34927487]\n",
      " [-0.03266674]]\n",
      "76 cost: 1.444207 hf: [[-0.8694466 ]\n",
      " [-0.8474238 ]\n",
      " [-0.7086204 ]\n",
      " [-0.5009417 ]\n",
      " [-0.66298896]\n",
      " [-0.50183624]\n",
      " [-0.34924728]\n",
      " [-0.03264003]]\n",
      "77 cost: 1.444103 hf: [[-0.86938596]\n",
      " [-0.8473663 ]\n",
      " [-0.70857257]\n",
      " [-0.5009048 ]\n",
      " [-0.662945  ]\n",
      " [-0.5017938 ]\n",
      " [-0.34921968]\n",
      " [-0.03261333]]\n",
      "78 cost: 1.4439988 hf: [[-0.8693253 ]\n",
      " [-0.84730864]\n",
      " [-0.70852464]\n",
      " [-0.50086784]\n",
      " [-0.662901  ]\n",
      " [-0.5017515 ]\n",
      " [-0.3491921 ]\n",
      " [-0.03258663]]\n",
      "79 cost: 1.4438949 hf: [[-0.8692646 ]\n",
      " [-0.8472511 ]\n",
      " [-0.7084767 ]\n",
      " [-0.500831  ]\n",
      " [-0.662857  ]\n",
      " [-0.50170916]\n",
      " [-0.34916455]\n",
      " [-0.03255993]]\n",
      "80 cost: 1.4437907 hf: [[-0.869204  ]\n",
      " [-0.84719336]\n",
      " [-0.70842886]\n",
      " [-0.50079405]\n",
      " [-0.662813  ]\n",
      " [-0.5016668 ]\n",
      " [-0.34913695]\n",
      " [-0.03253324]]\n",
      "81 cost: 1.4436865 hf: [[-0.8691433 ]\n",
      " [-0.8471357 ]\n",
      " [-0.708381  ]\n",
      " [-0.50075716]\n",
      " [-0.6627691 ]\n",
      " [-0.5016244 ]\n",
      " [-0.34910938]\n",
      " [-0.03250653]]\n",
      "82 cost: 1.4435825 hf: [[-0.8690827 ]\n",
      " [-0.8470782 ]\n",
      " [-0.7083331 ]\n",
      " [-0.50072026]\n",
      " [-0.6627251 ]\n",
      " [-0.501582  ]\n",
      " [-0.3490818 ]\n",
      " [-0.03247983]]\n",
      "83 cost: 1.4434785 hf: [[-0.869022  ]\n",
      " [-0.84702057]\n",
      " [-0.7082852 ]\n",
      " [-0.50068337]\n",
      " [-0.6626811 ]\n",
      " [-0.50153965]\n",
      " [-0.34905422]\n",
      " [-0.03245313]]\n",
      "84 cost: 1.4433745 hf: [[-0.86896133]\n",
      " [-0.846963  ]\n",
      " [-0.7082373 ]\n",
      " [-0.5006465 ]\n",
      " [-0.6626371 ]\n",
      " [-0.50149727]\n",
      " [-0.34902668]\n",
      " [-0.03242645]]\n",
      "85 cost: 1.4432704 hf: [[-0.8689007 ]\n",
      " [-0.8469054 ]\n",
      " [-0.7081894 ]\n",
      " [-0.5006096 ]\n",
      " [-0.6625931 ]\n",
      " [-0.5014549 ]\n",
      " [-0.34899908]\n",
      " [-0.03239974]]\n",
      "86 cost: 1.4431663 hf: [[-0.8688401 ]\n",
      " [-0.8468478 ]\n",
      " [-0.70814157]\n",
      " [-0.50057256]\n",
      " [-0.66254926]\n",
      " [-0.5014125 ]\n",
      " [-0.3489715 ]\n",
      " [-0.03237306]]\n",
      "87 cost: 1.4430623 hf: [[-0.8687794 ]\n",
      " [-0.84679025]\n",
      " [-0.7080937 ]\n",
      " [-0.5005357 ]\n",
      " [-0.6625052 ]\n",
      " [-0.50137013]\n",
      " [-0.34894395]\n",
      " [-0.03234635]]\n",
      "88 cost: 1.4429584 hf: [[-0.86871874]\n",
      " [-0.8467326 ]\n",
      " [-0.7080458 ]\n",
      " [-0.5004988 ]\n",
      " [-0.6624613 ]\n",
      " [-0.5013278 ]\n",
      " [-0.34891635]\n",
      " [-0.03231966]]\n",
      "89 cost: 1.4428543 hf: [[-0.8686581 ]\n",
      " [-0.84667504]\n",
      " [-0.7079979 ]\n",
      " [-0.50046194]\n",
      " [-0.6624173 ]\n",
      " [-0.50128543]\n",
      " [-0.34888878]\n",
      " [-0.03229298]]\n",
      "90 cost: 1.4427502 hf: [[-0.8685975 ]\n",
      " [-0.84661734]\n",
      " [-0.70795   ]\n",
      " [-0.500425  ]\n",
      " [-0.6623733 ]\n",
      " [-0.501243  ]\n",
      " [-0.34886122]\n",
      " [-0.03226629]]\n",
      "91 cost: 1.4426463 hf: [[-0.86853683]\n",
      " [-0.84655976]\n",
      " [-0.70790213]\n",
      " [-0.5003881 ]\n",
      " [-0.6623293 ]\n",
      " [-0.5012007 ]\n",
      " [-0.34883365]\n",
      " [-0.03223959]]\n",
      "92 cost: 1.4425422 hf: [[-0.86847615]\n",
      " [-0.8465022 ]\n",
      " [-0.7078543 ]\n",
      " [-0.5003512 ]\n",
      " [-0.6622853 ]\n",
      " [-0.5011583 ]\n",
      " [-0.34880608]\n",
      " [-0.03221291]]\n",
      "93 cost: 1.4424384 hf: [[-0.86841553]\n",
      " [-0.8464446 ]\n",
      " [-0.7078064 ]\n",
      " [-0.5003143 ]\n",
      " [-0.6622414 ]\n",
      " [-0.501116  ]\n",
      " [-0.3487785 ]\n",
      " [-0.03218623]]\n",
      "94 cost: 1.4423342 hf: [[-0.8683549 ]\n",
      " [-0.846387  ]\n",
      " [-0.7077585 ]\n",
      " [-0.50027734]\n",
      " [-0.6621974 ]\n",
      " [-0.5010736 ]\n",
      " [-0.34875098]\n",
      " [-0.03215956]]\n",
      "95 cost: 1.4422303 hf: [[-0.86829424]\n",
      " [-0.84632957]\n",
      " [-0.7077106 ]\n",
      " [-0.50024056]\n",
      " [-0.6621535 ]\n",
      " [-0.5010313 ]\n",
      " [-0.34872338]\n",
      " [-0.03213287]]\n",
      "96 cost: 1.4421265 hf: [[-0.8682336 ]\n",
      " [-0.84627193]\n",
      " [-0.70766276]\n",
      " [-0.5002036 ]\n",
      " [-0.6621096 ]\n",
      " [-0.50098896]\n",
      " [-0.3486958 ]\n",
      " [-0.03210619]]\n",
      "97 cost: 1.4420223 hf: [[-0.868173  ]\n",
      " [-0.8462144 ]\n",
      " [-0.7076149 ]\n",
      " [-0.5001667 ]\n",
      " [-0.6620656 ]\n",
      " [-0.5009465 ]\n",
      " [-0.34866828]\n",
      " [-0.03207953]]\n",
      "98 cost: 1.4419184 hf: [[-0.8681124 ]\n",
      " [-0.84615684]\n",
      " [-0.7075671 ]\n",
      " [-0.5001298 ]\n",
      " [-0.6620217 ]\n",
      " [-0.50090414]\n",
      " [-0.3486407 ]\n",
      " [-0.03205285]]\n",
      "99 cost: 1.4418145 hf: [[-0.8680517 ]\n",
      " [-0.84609926]\n",
      " [-0.70751923]\n",
      " [-0.50009286]\n",
      " [-0.6619777 ]\n",
      " [-0.5008618 ]\n",
      " [-0.34861314]\n",
      " [-0.03202617]]\n",
      "100 cost: 1.4417105 hf: [[-0.8679911 ]\n",
      " [-0.84604156]\n",
      " [-0.7074714 ]\n",
      " [-0.500056  ]\n",
      " [-0.6619337 ]\n",
      " [-0.5008195 ]\n",
      " [-0.3485856 ]\n",
      " [-0.03199949]]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(101):\n",
    "    cv,hv,_=sess.run([cost,hf,train],feed_dict={x:xdata,y:ydata})\n",
    "    print(step, \"cost:\",cv,\"hf:\",hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "#y값은 one hot 인코딩 방식으로 초기화 함...(a,b,c)중 하나를 hot하게 함...\n",
    "y_data = [[0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [1, 0, 0],#0\n",
    "          [1, 0, 0]]#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=3\n",
    "x=tf.placeholder(\"float\",[None,4])\n",
    "y=tf.placeholder(\"float\",[None,num_classes])\n",
    "w=tf.Variable(tf.random_normal([4,num_classes]))\n",
    "b=tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.nn.softmax( )  : 소프트 맥스 함수\n",
    "tensorflow에 nn(뉴럴네트워크)에 함수로 정의되어 있다\n",
    "\n",
    "* 시그모이드 함수처럼 0~1사이 값으로 만들어주고, \n",
    "* 총 합이 1이된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.nn.softmax(tf.matmul(x,w)+b) # --> None,3형태로 나오는 예측값을 y실제값과 비교해주면 된다\n",
    "cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.9959995\n",
      "200 0.63676447\n",
      "400 0.5324337\n",
      "600 0.4409767\n",
      "800 0.35128084\n",
      "1000 0.2638524\n",
      "1200 0.22489938\n",
      "1400 0.20489496\n",
      "1600 0.18802822\n",
      "1800 0.17362571\n",
      "2000 0.16119458\n",
      "**************************************************\n",
      "[1]\n",
      "**************************************************\n",
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# x1=([[1,11,7,9]])  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(train, feed_dict={x:x_data,y:y_data})\n",
    "        if step % 200==0:\n",
    "            print(step,sess.run(cost, feed_dict={x:x_data,y:y_data}))\n",
    "    print(\"*\"*50)\n",
    "    # x_data = 1,11,7,9 일때 무엇을 예측할지?\n",
    "    res = (sess.run(hf, feed_dict={x:[[1,11,7,9]]}))\n",
    "    print(sess.run(tf.argmax(res,axis=1)))\n",
    "    \n",
    "    print(\"*\"*50)\n",
    "    res2 = (sess.run(hf, feed_dict={x:[[1,11,7,9],\n",
    "                                      [1,3,4,3],\n",
    "                                      [1,1,0,1]]}))\n",
    "    print(sess.run(tf.argmax(res2,axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 17)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy=np.loadtxt(\"/Users/user/Downloads/zoo.csv\",delimiter=',',dtype=np.float32)\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata=xy[:,0:-1] \n",
    "ydata=xy[:,[-1]] \n",
    "xdata.shape\n",
    "ydata.shape         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_1:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshaped_one_hot Tensor(\"Reshape_1:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,16])\n",
    "y=tf.placeholder(tf.int32,shape=[None,1])\n",
    "# onehot()를 통해 y값을 원핫인코딩해줌\n",
    "y_one_hot = tf.one_hot(y,7) #3번 동물 --> 3(y) --> 0001000\n",
    "print(\"one_hot\",y_one_hot)\n",
    "# reshape을 통해 2차원으로 줄여줌. \n",
    "y_one_hot=tf.reshape(y_one_hot,[-1,7])\n",
    "print(\"reshaped_one_hot\",y_one_hot)\n",
    "# one hot 함수는 한 차원 높게 변환된다\n",
    "# y :[[0],[3],...[5]] =>[None,1]\n",
    "# onehot :[[[1000000]],[[0001000]], ...[[0000010]]=> [None,1,7]\n",
    "# but, 우리가 원하는 shape은 [None,7]이다\n",
    "# reshape을 사용하여 tf.shape(Y_ond_hot,[-1,7])을 사용하여 [None,7]로 reshape 해준다\n",
    "\n",
    "# 0~6 : 7가지 종류의 동물 --> 분류기 7개 필요\n",
    "# w1x1+w2x2+...+w7x7+b --> softmax => 0번 종류의 동물에 대한 확률\n",
    "# w1x1+w2x2+...+w7x7+b --> softmax => 1번 종류의 동물에 대한 확률\n",
    "# w1x1+w2x2+...+w7x7+b --> softmax => 2번 종류의 동물에 대한 확률\n",
    "# ....\n",
    "# w1x1+w2x2+...+w7x7+b --> softmax => 6번 종류의 동물에 대한 확률\n",
    "\n",
    "\n",
    "# w=tf.Variable(tf.random_normal([16,1]))\n",
    "# b=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.random_normal([16,7]))\n",
    "b=tf.Variable(tf.random_normal([7]))\n",
    "\n",
    "logit=tf.matmul(x,w)+b\n",
    "# hf는 softmax를 사용하여 y값을 확률로 바꿔줌 --> 7개 값이 나온다\n",
    "hf= tf.nn.softmax(logit)\n",
    "cost=tf.nn.softmax_cross_entropy_with_logits(logits=logit,labels=y_one_hot)\n",
    "cost2=tf.reduce_mean(cost)\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost2)\n",
    "# hf를 통해 출력된 값 중 가장 큰 변환기의 넘버를 리턴\n",
    "prediction=tf.argmax(hf,1) \n",
    "# 원핫인코딩하여 저장된 y값의 최댓값과 비교하는 변수\n",
    "correct_prediction=tf.equal(prediction, tf.argmax(y_one_hot,1)) \n",
    "# 정확도를 알아보기 위해 구한 정확도의 평균을 리턴한다\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 0 accuracy: [5.2561336, 0.14851485]\n",
      "100 cost: 100 accuracy: [0.71324164, 0.7326733]\n",
      "200 cost: 200 accuracy: [0.39250928, 0.8811881]\n",
      "300 cost: 300 accuracy: [0.28499764, 0.9306931]\n",
      "400 cost: 400 accuracy: [0.2276001, 0.9405941]\n",
      "500 cost: 500 accuracy: [0.19096999, 0.96039605]\n",
      "600 cost: 600 accuracy: [0.16507408, 0.96039605]\n",
      "700 cost: 700 accuracy: [0.14556673, 0.980198]\n",
      "800 cost: 800 accuracy: [0.13024046, 0.980198]\n",
      "900 cost: 900 accuracy: [0.11783738, 0.980198]\n",
      "1000 cost: 1000 accuracy: [0.10757683, 0.990099]\n",
      "1100 cost: 1100 accuracy: [0.09894205, 0.990099]\n",
      "1200 cost: 1200 accuracy: [0.091573864, 0.990099]\n",
      "1300 cost: 1300 accuracy: [0.085213445, 0.990099]\n",
      "1400 cost: 1400 accuracy: [0.0796685, 1.0]\n",
      "1500 cost: 1500 accuracy: [0.074792996, 1.0]\n",
      "1600 cost: 1600 accuracy: [0.070473894, 1.0]\n",
      "1700 cost: 1700 accuracy: [0.06662204, 1.0]\n",
      "1800 cost: 1800 accuracy: [0.0631665, 1.0]\n",
      "1900 cost: 1900 accuracy: [0.060049757, 1.0]\n",
      "2000 cost: 2000 accuracy: [0.05722489, 1.0]\n",
      "2100 cost: 2100 accuracy: [0.054653235, 1.0]\n",
      "2200 cost: 2200 accuracy: [0.05230256, 1.0]\n",
      "2300 cost: 2300 accuracy: [0.050145857, 1.0]\n",
      "2400 cost: 2400 accuracy: [0.048160292, 1.0]\n",
      "2500 cost: 2500 accuracy: [0.04632644, 1.0]\n",
      "2600 cost: 2600 accuracy: [0.044627704, 1.0]\n",
      "2700 cost: 2700 accuracy: [0.04304977, 1.0]\n",
      "2800 cost: 2800 accuracy: [0.041580323, 1.0]\n",
      "2900 cost: 2900 accuracy: [0.04020853, 1.0]\n",
      "3000 cost: 3000 accuracy: [0.038925115, 1.0]\n",
      "3100 cost: 3100 accuracy: [0.037721753, 1.0]\n",
      "3200 cost: 3200 accuracy: [0.03659125, 1.0]\n",
      "3300 cost: 3300 accuracy: [0.035527185, 1.0]\n",
      "3400 cost: 3400 accuracy: [0.03452389, 1.0]\n",
      "3500 cost: 3500 accuracy: [0.033576295, 1.0]\n",
      "3600 cost: 3600 accuracy: [0.03267992, 1.0]\n",
      "3700 cost: 3700 accuracy: [0.03183071, 1.0]\n",
      "3800 cost: 3800 accuracy: [0.031024959, 1.0]\n",
      "3900 cost: 3900 accuracy: [0.030259553, 1.0]\n",
      "4000 cost: 4000 accuracy: [0.029531438, 1.0]\n",
      "4100 cost: 4100 accuracy: [0.028837943, 1.0]\n",
      "4200 cost: 4200 accuracy: [0.028176734, 1.0]\n",
      "4300 cost: 4300 accuracy: [0.02754554, 1.0]\n",
      "4400 cost: 4400 accuracy: [0.026942361, 1.0]\n",
      "4500 cost: 4500 accuracy: [0.026365396, 1.0]\n",
      "4600 cost: 4600 accuracy: [0.025812898, 1.0]\n",
      "4700 cost: 4700 accuracy: [0.025283422, 1.0]\n",
      "4800 cost: 4800 accuracy: [0.024775485, 1.0]\n",
      "4900 cost: 4900 accuracy: [0.024287868, 1.0]\n",
      "5000 cost: 5000 accuracy: [0.0238193, 1.0]\n",
      "5100 cost: 5100 accuracy: [0.023368694, 1.0]\n",
      "5200 cost: 5200 accuracy: [0.022935098, 1.0]\n",
      "5300 cost: 5300 accuracy: [0.02251747, 1.0]\n",
      "5400 cost: 5400 accuracy: [0.022114983, 1.0]\n",
      "5500 cost: 5500 accuracy: [0.02172682, 1.0]\n",
      "5600 cost: 5600 accuracy: [0.021352272, 1.0]\n",
      "5700 cost: 5700 accuracy: [0.020990558, 1.0]\n",
      "5800 cost: 5800 accuracy: [0.020641051, 1.0]\n",
      "5900 cost: 5900 accuracy: [0.02030316, 1.0]\n",
      "6000 cost: 6000 accuracy: [0.019976292, 1.0]\n",
      "6100 cost: 6100 accuracy: [0.019659908, 1.0]\n",
      "6200 cost: 6200 accuracy: [0.01935355, 1.0]\n",
      "6300 cost: 6300 accuracy: [0.019056704, 1.0]\n",
      "6400 cost: 6400 accuracy: [0.018768957, 1.0]\n",
      "6500 cost: 6500 accuracy: [0.01848984, 1.0]\n",
      "6600 cost: 6600 accuracy: [0.018219031, 1.0]\n",
      "6700 cost: 6700 accuracy: [0.017956156, 1.0]\n",
      "6800 cost: 6800 accuracy: [0.017700868, 1.0]\n",
      "6900 cost: 6900 accuracy: [0.017452791, 1.0]\n",
      "7000 cost: 7000 accuracy: [0.017211696, 1.0]\n",
      "7100 cost: 7100 accuracy: [0.016977206, 1.0]\n",
      "7200 cost: 7200 accuracy: [0.016749144, 1.0]\n",
      "7300 cost: 7300 accuracy: [0.01652717, 1.0]\n",
      "7400 cost: 7400 accuracy: [0.016311092, 1.0]\n",
      "7500 cost: 7500 accuracy: [0.016100675, 1.0]\n",
      "7600 cost: 7600 accuracy: [0.015895657, 1.0]\n",
      "7700 cost: 7700 accuracy: [0.015695889, 1.0]\n",
      "7800 cost: 7800 accuracy: [0.015501102, 1.0]\n",
      "7900 cost: 7900 accuracy: [0.015311147, 1.0]\n",
      "8000 cost: 8000 accuracy: [0.015125876, 1.0]\n",
      "8100 cost: 8100 accuracy: [0.014945099, 1.0]\n",
      "8200 cost: 8200 accuracy: [0.014768639, 1.0]\n",
      "8300 cost: 8300 accuracy: [0.014596333, 1.0]\n",
      "8400 cost: 8400 accuracy: [0.014428033, 1.0]\n",
      "8500 cost: 8500 accuracy: [0.014263638, 1.0]\n",
      "8600 cost: 8600 accuracy: [0.01410299, 1.0]\n",
      "8700 cost: 8700 accuracy: [0.013945962, 1.0]\n",
      "8800 cost: 8800 accuracy: [0.013792441, 1.0]\n",
      "8900 cost: 8900 accuracy: [0.013642293, 1.0]\n",
      "9000 cost: 9000 accuracy: [0.013495407, 1.0]\n",
      "9100 cost: 9100 accuracy: [0.013351692, 1.0]\n",
      "9200 cost: 9200 accuracy: [0.013211069, 1.0]\n",
      "9300 cost: 9300 accuracy: [0.013073353, 1.0]\n",
      "9400 cost: 9400 accuracy: [0.012938565, 1.0]\n",
      "9500 cost: 9500 accuracy: [0.0128065245, 1.0]\n",
      "9600 cost: 9600 accuracy: [0.012677201, 1.0]\n",
      "9700 cost: 9700 accuracy: [0.012550499, 1.0]\n",
      "9800 cost: 9800 accuracy: [0.012426287, 1.0]\n",
      "9900 cost: 9900 accuracy: [0.012304584, 1.0]\n",
      "10000 cost: 10000 accuracy: [0.01218527, 1.0]\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:4 실제y:4\n",
      "[True]예측:4 실제y:4\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:4 실제y:4\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:2 실제y:2\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:2 실제y:2\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:2 실제y:2\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:4 실제y:4\n",
      "[True]예측:2 실제y:2\n",
      "[True]예측:2 실제y:2\n",
      "[True]예측:3 실제y:3\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:1 실제y:1\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:5 실제y:5\n",
      "[True]예측:0 실제y:0\n",
      "[True]예측:6 실제y:6\n",
      "[True]예측:1 실제y:1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x:xdata,y:ydata})\n",
    "        if step % 100==0:\n",
    "            cv,av=step,sess.run([cost2,accuracy], feed_dict={x:xdata,y:ydata})\n",
    "            print(step,\"cost:\",cv,\"accuracy:\",av)\n",
    "            ###########모델을 만들었다##############\n",
    "# 전체 데이터로 트레이닝 수행하여 모델 생성\n",
    "# 모델에 전체 데이터를 넣어 정확도 출력\n",
    "# 데이터를 분할하지 않았음\n",
    "    pred=sess.run(prediction,feed_dict={x:xdata,y:ydata})\n",
    "    for p,y in zip(pred, ydata.flatten()):\n",
    "        print(\"[{}]예측:{} 실제y:{}\".format(p==int(y),p,int(y)))\n",
    "    # [[0],[3],...] --> [0,3,...]\n",
    "#     print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten()\n",
    "[[1],[0]] --> [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
