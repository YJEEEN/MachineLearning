{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting \n",
    "* training data에 과적합(너무 최적화) 된 상태로 weight 값이 결정된 상태\n",
    "\n",
    "  직선아닌 곡선의 형태로 그래프가 그려진다 --> n차원으로 갈수록 많이 구불거린다\n",
    "  high Variance 상태\n",
    "* 일반화된 상태가 가장 적당하다\n",
    "    - Regularization:정규화(모든 특성은 사용하되, 고차원의 weight를 크게 설정함으로써 \n",
    "          고차원feature가 작아져서 영향력이 줄어드는 --> 구불거리는 함수를 flat하게 만들어준다)\n",
    "    - feature를 줄인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images,train_labels),(test_images,test_labels)=mnist.load_data()\n",
    "train_images.shape\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models 패키지의 Sequential 객체 생성\n",
    "network=models.Sequential()\n",
    "# 객체.add로 계층을 추가\n",
    "# 이 계층은 layers라는 keras의 서브패키지의 Dense객체를 사용하면 된다\n",
    "# Dense(크기,activation='함수명',입력데이터 shape=(n,n,데이터크기는안써줘도된다))\n",
    "network.add(layers.Dense(512,activation='relu',input_shape=(28*28,))) # 60000,784\n",
    "network.add(layers.Dense(10,activation='softmax')) # input_shape은 맨 처음만 선언해주면된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile 과정 : 훈련을 위한 정의부\n",
    "- cost함수, Optimizer종류 등 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss: 성능관련, optimizer: 업데이트 관련\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network 설계 후, 파라미터로 넘겨준 지표로 계층이 compile된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화 이야기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) 의미 : 최적화란 cost를 최소화하는 방향으로 parameter를 update하는 것\n",
    "2) 종류\n",
    "   - 경사하강법(SGD):기울기*학습률(learning rate=step size) => 가중치를 갱신\n",
    "                     <단점> 최적의 기울기를 구하는데 적합하지않고, 시간이 걸린다\n",
    "                            단, 모델이 단순하다면 괜찮다\n",
    "                \n",
    "   - 모멘텀 : 운동량(기울기&속도-->마찰로인한 속도저하)\n",
    "                속도가 크면 => 기울기를 크게 update하여 (제곱하여 더함) 학습률을 낮춘다\n",
    "                <단점> 학습률이 안좋다는 단점\n",
    "\n",
    "   - AdaGrad(RMSProp) : 제곱하는 값이 커지다보면 update가 힘들다\n",
    "               학습률이 크면 발산이되고 작으면 업데이트가 너무 느리다\n",
    "               처음엔 학습률크게 나중에 갈수록 학습률을 작게 조율하는(모멘텀방식)\n",
    "               =>RMSProp은 새로운 기울기만 학습률에 반영\n",
    "                학습률을 동일하게 주지말고, 상황에 따라서 반영하자는 \n",
    "\n",
    "   - Adam(모멘텀+Ada) : 학습에 따라서 갱신의 정도를 조율하자\n",
    "    \n",
    "   ==> 현재까지 연구된 바에 의하면, Adam을 쓰는 것이 제일 무난한다.(적합한 Optimizer를 못찾겠다면)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 보통 int 타입으로 들어가있으므로, float32로 바꿔주어야 한다.\n",
    "# 정규화 해야한다면, ('float32')/255 로 수행하여야 한다\n",
    "train_images=train_images.reshape((60000,28*28))\n",
    "train_images=train_images.astype('float32')/255\n",
    "\n",
    "test_images=test_images.reshape((10000,28*28))\n",
    "test_images=test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, test의 labels가 0-9까지의 숫자로 저장되어있다\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 11:15:27.565844 12180 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0807 11:15:27.622871 12180 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2593 - acc: 0.9253\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1062 - acc: 0.9687: 0s - loss: 0.1\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 58us/step - loss: 0.0703 - acc: 0.9788\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0503 - acc: 0.9850\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0382 - acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x261a7306c18>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0-9까지의 분류기로 원핫인코딩하기 위해 카테고리화 시킨다\n",
    "train_labels=to_categorical(train_labels)\n",
    "test_labels=to_categorical(test_labels)\n",
    "\n",
    "network.fit(train_images,train_labels,epochs=5,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 45us/step\n"
     ]
    }
   ],
   "source": [
    "# test data에 대해 평가하는 함수\n",
    "test_cost, test_acc = network.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9796\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영문 자연어 처리 패키지\n",
    "import nltk\n",
    "# 단어단위로 토큰화 작업을 수행하는 함수\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'are', 'you', '?']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('아버지', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('방', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('들어가신다', 'Verb')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "okt.pos(\"아버지가 방에 들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', \"'s\", ',', 'up', '?', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"what's, up?.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', \"'s\", ',', 'up', '?', '.']\n",
      "['what', \"'\", 's', ',', 'up', '?.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(word_tokenize(\"what's, up?.\"))\n",
    "print(WordPunctTokenizer().tokenize(\"what's, up?.\")) # 't 's를 따로 분류해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[('롯데마트', 'ncn'), ('의', 'jcm')], [('롯데마트의', 'ncn')], [('롯데마트', 'nqq'), ('의', 'jcm')], [('롯데마트의', 'nqq')]], [[('흑마늘', 'ncn')], [('흑마늘', 'nqq')]], [[('양념', 'ncn')]], [[('치킨', 'ncn'), ('이', 'jcc')], [('치킨', 'ncn'), ('이', 'jcs')], [('치킨', 'ncn'), ('이', 'ncn')]], [[('논란', 'ncpa'), ('이', 'jcc')], [('논란', 'ncpa'), ('이', 'jcs')], [('논란', 'ncpa'), ('이', 'ncn')]], [[('되', 'nbu'), ('고', 'jcj')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecc')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecs')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecx')], [('되', 'paa'), ('고', 'ecc')], [('되', 'paa'), ('고', 'ecs')], [('되', 'paa'), ('고', 'ecx')], [('되', 'pvg'), ('고', 'ecc')], [('되', 'pvg'), ('고', 'ecs')], [('되', 'pvg'), ('고', 'ecx')], [('되', 'px'), ('고', 'ecc')], [('되', 'px'), ('고', 'ecs')], [('되', 'px'), ('고', 'ecx')]], [[('있', 'paa'), ('다', 'ef')], [('있', 'px'), ('다', 'ef')]], [[('.', 'sf')], [('.', 'sy')]]]\n"
     ]
    }
   ],
   "source": [
    "hannanum = Hannanum()\n",
    "print(hannanum.analyze('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is an interpreted, high-level, general-purpose programming language.', \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']\n",
      "====================================================================================================\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.', 'Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']\n",
      "====================================================================================================\n",
      "[('Python', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('interpreted', 'JJ'), (',', ','), ('high-level', 'JJ'), (',', ','), ('general-purpose', 'JJ'), ('programming', 'NN'), ('language', 'NN'), ('.', '.'), ('Created', 'VBN'), ('by', 'IN'), ('Guido', 'NNP'), ('van', 'NN'), ('Rossum', 'NNP'), ('and', 'CC'), ('first', 'RB'), ('released', 'VBN'), ('in', 'IN'), ('1991', 'CD'), (',', ','), ('Python', 'NNP'), (\"'s\", 'POS'), ('design', 'NN'), ('philosophy', 'NN'), ('emphasizes', 'VBZ'), ('code', 'JJ'), ('readability', 'NN'), ('with', 'IN'), ('its', 'PRP$'), ('notable', 'JJ'), ('use', 'NN'), ('of', 'IN'), ('significant', 'JJ'), ('whitespace', 'NN'), ('.', '.'), ('Its', 'PRP$'), ('language', 'NN'), ('constructs', 'NNS'), ('and', 'CC'), ('object-oriented', 'JJ'), ('approach', 'NN'), ('aim', 'NN'), ('to', 'TO'), ('help', 'VB'), ('programmers', 'NNS'), ('write', 'VB'), ('clear', 'JJ'), (',', ','), ('logical', 'JJ'), ('code', 'NN'), ('for', 'IN'), ('small', 'JJ'), ('and', 'CC'), ('large-scale', 'JJ'), ('projects', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import *\n",
    "from nltk.tag import *\n",
    "text=\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "print(sent_tokenize(text)) # '.' 마침표 기준으로 문장을 분리해준다\n",
    "print(\"=\"*100)\n",
    "print(word_tokenize(text)) # 단어로 분리해준다\n",
    "print(\"=\"*100)\n",
    "print(pos_tag(word_tokenize(text))) # 품사를 같이 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['집가고', '싶당']\n",
      "====================================================================================================\n",
      "['집', '가', '고', '싶', '다']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(okt.morphs(\"집가고 싶당\"))\n",
    "print(\"=\"*100)\n",
    "\n",
    "kma=Kkma()\n",
    "print(kma.morphs(\"집가고 싶당\"))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리에 많이쓰이는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "wnl=WordNetLemmatizer()\n",
    "print(wnl.lemmatize(\"has\",\"v\"))\n",
    "print(wnl.lemmatize(\"were\",\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 포터알고리즘(어간 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.', 'Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']\n",
      "================================================================================\n",
      "['python', 'is', 'an', 'interpret', ',', 'high-level', ',', 'general-purpos', 'program', 'languag', '.', 'creat', 'by', 'guido', 'van', 'rossum', 'and', 'first', 'releas', 'in', '1991', ',', 'python', \"'s\", 'design', 'philosophi', 'emphas', 'code', 'readabl', 'with', 'it', 'notabl', 'use', 'of', 'signific', 'whitespac', '.', 'it', 'languag', 'construct', 'and', 'object-ori', 'approach', 'aim', 'to', 'help', 'programm', 'write', 'clear', ',', 'logic', 'code', 'for', 'small', 'and', 'large-scal', 'project', '.']\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(text)\n",
    "print(words)\n",
    "print(\"=\"*80)\n",
    "ps_words=[ps.stem(w) for w in words ]\n",
    "print(ps_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 패키지(필요할때 사용하면 된다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw=stopwords.words('english')\n",
    "sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 사전에 있는 단어를 제외하고 싶다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'help', '.', 'like', 'coding', '.', \"'s\", 'hobby', '?']\n"
     ]
    }
   ],
   "source": [
    "test=\"i need your help. i like coding. what's your hobby?\"\n",
    "test=word_tokenize(test)\n",
    "test\n",
    "res=[]\n",
    "for w in test:\n",
    "    if w not in sw:\n",
    "        res.append(w)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['파이썬', '코딩을', '해야', '합니다', '.', '하기싫어도', '해요', '.', '그래도', '하세요']\n"
     ]
    }
   ],
   "source": [
    "sw=\"열심히 하기싫어 싫거든 안해\"\n",
    "test=\"파이썬 코딩을 열심히 해야 합니다. 하기싫어도 해요. 그래도 싫거든 하세요 안해 안해 안해\"\n",
    "\n",
    "sw=sw.split(\" \") # split은 문자로 나눔\n",
    "sw\n",
    "test=word_tokenize(test) # 단순 split아니라 .,과 같은 것을 구분\n",
    "res2=[]\n",
    "for w in test:\n",
    "    if w not in sw:\n",
    "        res2.append(w)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩\n",
    "text\n",
    "# 각 단어에 대한 등장횟수 출력\n",
    "# 단어 길이가 2이하인 경우 제외\n",
    "# 불용어 사전의 단어 제거\n",
    "# 대소문자 구분 없음 (모두 소문자로 변환) \"Have\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'created', 'guido', 'van', 'rossum', 'first', 'released', '1991', ',', 'python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace', '.', 'language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'small', 'large-scale', 'projects', '.']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 사전 가져오기\n",
    "sw=stopwords.words('english')\n",
    "sw\n",
    "\n",
    "# 대소문자 -> 소문자로 통일\n",
    "text=text.lower()\n",
    "\n",
    "# 단어 단위로 저장\n",
    "text =word_tokenize(text)\n",
    "res_text=[]\n",
    "\n",
    "# 불용어 제거 \n",
    "for w in text:\n",
    "    if w not in sw:\n",
    "        res_text.append(w)\n",
    "print(res_text)\n",
    "\n",
    "# 반복되는 단어 숫자세기\n",
    "from collections import Counter\n",
    "cnt=0\n",
    "for i in range(len(res_text)-1):\n",
    "    for k in range(len(res_text))\n",
    "    if res_text[i]=="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
